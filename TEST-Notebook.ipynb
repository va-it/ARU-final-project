{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the Jupyter Notebook for the MAGICODE project - TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the volumes together into a single zip file\n",
    "!zip -s 0 dataset.zip -O dataset_joined.zip\n",
    "# unzip the newly assembled archive\n",
    "!unzip dataset_joined.zip -d ./all_data\n",
    "print ('files unzipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into training and evaluation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "import glob\n",
    "\n",
    "# define source folder\n",
    "source = 'all_data/dataset'\n",
    "# get all file paths\n",
    "all_files = os.listdir(source)\n",
    "# build a generic image path (e.g. 'all_data/dataset/*.png')\n",
    "images_path = os.path.join(source, '*.png')\n",
    "# get all images paths\n",
    "img_files = glob.glob(images_path)\n",
    "\n",
    "# training set will be 6 times the size of the evaluation set\n",
    "distribution=6\n",
    "TRAINING_SET_NAME = \"training_set\"\n",
    "EVALUATION_SET_NAME = \"eval_set\"\n",
    "\n",
    "# splits randomly the files into two sets\n",
    "train_set,eval_set = train_test_split(img_files, train_size=(distribution / 10))\n",
    "\n",
    "# copy the files (img and gui) from the all_data folder into the training_set folder\n",
    "for file in train_set:\n",
    "    shutil.copyfile(\"{}/{}.png\".format(source, file), \"{}/{}/{}.png\".format(os.path.dirname(source), TRAINING_SET_NAME, file))\n",
    "    shutil.copyfile(\"{}/{}.gui\".format(source, file), \"{}/{}/{}.png\".format(os.path.dirname(source), TRAINING_SET_NAME, file))\n",
    "\n",
    "# copy the files (img and gui) from the all_data folder into the eval_set folder\n",
    "for file in eval_set:\n",
    "    shutil.copyfile(\"{}/{}.png\".format(source, file), \"{}/{}/{}.png\".format(os.path.dirname(source), EVALUATION_SET_NAME, file))\n",
    "    shutil.copyfile(\"{}/{}.gui\".format(source, file), \"{}/{}/{}.png\".format(os.path.dirname(source), EVALUATION_SET_NAME, file))\n",
    "\n",
    "print(\"Training dataset: {}/training_set\".format(os.path.dirname(source), path))\n",
    "print(\"Evaluation dataset: {}/eval_set\".format(os.path.dirname(source), path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transoform training set into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d54766e7a1c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".png\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_preprocessed_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#define source and destination folders\n",
    "source = 'all_data/dataset/training_set'\n",
    "destination = 'all_data/training_features\n",
    "\n",
    "# transform images in training dataset (i.e. normalized pixel values and resized pictures) to numpy arrays (smaller files, useful if uploading the set to train a model in the cloud)\n",
    "for f in os.listdir(source):\n",
    "    if f.find(\".png\") != -1:\n",
    "        img = Utils.get_preprocessed_img(\"{}/{}\".format(source, f), IMAGE_SIZE)\n",
    "        file_name = f[:f.find(\".png\")]\n",
    "\n",
    "        np.savez_compressed(\"{}/{}\".format(destination, file_name), features=img)\n",
    "        retrieve = np.load(\"{}/{}.npz\".format(destination, file_name))[\"features\"]\n",
    "\n",
    "        assert np.array_equal(img, retrieve)\n",
    "\n",
    "        shutil.copyfile(\"{}/{}.gui\".format(source, file_name), \"{}/{}.gui\".format(destination, file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "if not os.path.exists('bin'):\n",
    "    os.mkdir('bin')\n",
    "os.chdir('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using a generator (to avoid having to fit all the data in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "\n",
    "import sys\n",
    "\n",
    "from classes.dataset.Generator import *\n",
    "from classes.model.pix2code import *\n",
    "\n",
    "np.random.seed(1234)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.load(input_path, generate_binary_sequences=True)\n",
    "dataset.save_metadata(output_path)\n",
    "dataset.voc.save(output_path)\n",
    "\n",
    "gui_paths, img_paths = Dataset.load_paths_only(input_path)\n",
    "\n",
    "input_shape = dataset.input_shape\n",
    "output_size = dataset.output_size\n",
    "steps_per_epoch = dataset.size / BATCH_SIZE\n",
    "voc = Vocabulary()\n",
    "voc.retrieve(output_path)\n",
    "\n",
    "generator = Generator.data_generator(voc, gui_paths, img_paths, batch_size=BATCH_SIZE, generate_binary_sequences=True)\n",
    "\n",
    "model.fit_generator(generator, steps_per_epoch=steps_per_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
