{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the Jupyter Notebook for the MAGICODE project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First install some modules that might not be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "# install sklearn to use train_test_split function\n",
    "!{sys.executable} -m pip install sklearn\n",
    "# install opencv to use cv2 module in get_preprocessed_img\n",
    "!{sys.executable} -m pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then import some libraries and modules that are needed for the code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn.model_selection as model_selection\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the volumes together into a single zip file\n",
    "!zip -s 0 dataset.zip -O dataset_joined.zip\n",
    "# unzip the newly assembled archive into the current folder\n",
    "!unzip dataset_joined.zip -d ./\n",
    "\n",
    "if os.listdir('dataset') :\n",
    "    print ('files unzipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some values used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SET_NAME = 'training_set'\n",
    "EVALUATION_SET_NAME = 'eval_set'\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "CONTEXT_LENGTH = 48\n",
    "START_TOKEN = \"<START>\"\n",
    "END_TOKEN = \"<END>\"\n",
    "PLACEHOLDER = \" \"\n",
    "SEPARATOR = '->'\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into training and evaluation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define source folder\n",
    "source = 'dataset'\n",
    "# get all file paths\n",
    "all_files = os.listdir(source)\n",
    "# build a generic image path (e.g. 'all_data/dataset/*.png')\n",
    "images_path = join(source, '*.png')\n",
    "# get all images paths\n",
    "img_files = glob.glob(images_path)\n",
    "# remove files extension from files paths\n",
    "img_files_without_extension = [Path(img_file).stem for img_file in img_files]\n",
    "\n",
    "# splits randomly the files into two sets (train_set = 85% of dataset, eval_set = 15% of dataset)\n",
    "train_set,eval_set = model_selection.train_test_split(img_files_without_extension, train_size=0.85)\n",
    "\n",
    "# create the TRAINING_SET_NAME and EVALUATION_SET_NAME directories if they do not exist\n",
    "if not os.path.exists(join(source, TRAINING_SET_NAME)):\n",
    "    os.makedirs(join(source, TRAINING_SET_NAME))\n",
    "if not os.path.exists(join(source, EVALUATION_SET_NAME)):\n",
    "    os.makedirs(join(source, EVALUATION_SET_NAME))\n",
    "\n",
    "# copy the files (img and gui) from the all_data folder into the training_set folder\n",
    "for file in train_set:\n",
    "    shutil.copyfile(join(source, file + '.png'), join(source, TRAINING_SET_NAME, file + '.png'))\n",
    "    shutil.copyfile(join(source, file + '.gui'), join(source, TRAINING_SET_NAME, file + '.gui'))\n",
    "\n",
    "# copy the files (img and gui) from the all_data folder into the eval_set folder\n",
    "for file in eval_set:\n",
    "    shutil.copyfile(join(source, file + '.png'), join(source, EVALUATION_SET_NAME, file + '.png'))\n",
    "    shutil.copyfile(join(source, file + '.gui'), join(source, EVALUATION_SET_NAME, file + '.gui'))\n",
    "\n",
    "print('Training dataset: {}'.format(join(source, TRAINING_SET_NAME)))\n",
    "print('Evaluation dataset: {}'.format(join(source, EVALUATION_SET_NAME)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some Classes and functions that will be used a few times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform training set into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define source and destination folders\n",
    "source = join('dataset', 'training_set')\n",
    "destination = join('dataset', 'training_features')\n",
    "\n",
    "# create the training_features directory if it does not exist\n",
    "if not os.path.exists(destination):\n",
    "    os.makedirs(destination)\n",
    "\n",
    "# transform images in training dataset (i.e. normalized pixel values and resized pictures) to numpy arrays (smaller files, useful if uploading the set to train a model in the cloud)\n",
    "for f in os.listdir(source):\n",
    "    if f.find('.png') != -1:\n",
    "        img = Utils.get_preprocessed_img(join(source, f), IMAGE_SIZE)\n",
    "        file_name = f[:f.find('.png')]\n",
    "\n",
    "        np.savez_compressed(join(destination, file_name), features=img)\n",
    "        retrieve = np.load(join(destination, file_name + '.npz'))['features']\n",
    "        \n",
    "        assert np.array_equal(img, retrieve)\n",
    "        \n",
    "        shutil.copyfile(join(source, file_name + '.gui'), join(destination, file_name + '.gui'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make folder to store runtime files\n",
    "if not os.path.exists('bin'):\n",
    "    os.mkdir('bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare magicode class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, \\\n",
    "                         RepeatVector, LSTM, concatenate, \\\n",
    "                         Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Sequential, Model, model_from_json\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras import *\n",
    "\n",
    "class magicode:\n",
    "    def __init__(self, input_shape, output_size, output_path):\n",
    "        self.model = None\n",
    "        self.name = 'magicode'\n",
    "        self.input_shape = input_shape\n",
    "        self.output_size = output_size\n",
    "        self.output_path = output_path\n",
    "\n",
    "        image_model = Sequential()\n",
    "        image_model.add(Conv2D(32, (3, 3), padding='valid', activation='relu', input_shape=input_shape))\n",
    "        image_model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))\n",
    "        image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        image_model.add(Dropout(0.25))\n",
    "\n",
    "        image_model.add(Conv2D(64, (3, 3), padding='valid', activation='relu'))\n",
    "        image_model.add(Conv2D(64, (3, 3), padding='valid', activation='relu'))\n",
    "        image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        image_model.add(Dropout(0.25))\n",
    "\n",
    "        image_model.add(Conv2D(128, (3, 3), padding='valid', activation='relu'))\n",
    "        image_model.add(Conv2D(128, (3, 3), padding='valid', activation='relu'))\n",
    "        image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        image_model.add(Dropout(0.25))\n",
    "\n",
    "        image_model.add(Flatten())\n",
    "        image_model.add(Dense(1024, activation='relu'))\n",
    "        image_model.add(Dropout(0.3))\n",
    "        image_model.add(Dense(1024, activation='relu'))\n",
    "        image_model.add(Dropout(0.3))\n",
    "\n",
    "        image_model.add(RepeatVector(CONTEXT_LENGTH))\n",
    "\n",
    "        visual_input = Input(shape=input_shape)\n",
    "        encoded_image = image_model(visual_input)\n",
    "\n",
    "        language_model = Sequential()\n",
    "        language_model.add(LSTM(128, return_sequences=True, input_shape=(CONTEXT_LENGTH, output_size)))\n",
    "        language_model.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "        textual_input = Input(shape=(CONTEXT_LENGTH, output_size))\n",
    "        encoded_text = language_model(textual_input)\n",
    "\n",
    "        decoder = concatenate([encoded_image, encoded_text])\n",
    "\n",
    "        decoder = LSTM(512, return_sequences=True)(decoder)\n",
    "        decoder = LSTM(512, return_sequences=False)(decoder)\n",
    "        decoder = Dense(output_size, activation='softmax')(decoder)\n",
    "\n",
    "        self.model = Model(inputs=[visual_input, textual_input], outputs=decoder)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.0001, clipvalue=1.0)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def fit_generator(self, generator, steps_per_epoch):\n",
    "        self.model.fit(generator, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, verbose=1)\n",
    "        self.save()\n",
    "\n",
    "    def predict(self, image, partial_caption):\n",
    "        return self.model.predict([image, partial_caption], verbose=0)[0]\n",
    "\n",
    "    def save(self):\n",
    "        model_json = self.model.to_json()\n",
    "        with open(format(self.output_path, self.name, '.json'), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        self.model.save_weights(join(self.output_path, self.name, '.h5'))\n",
    "\n",
    "    def load(self, name=\"\"):\n",
    "        output_name = self.name if name == \"\" else name\n",
    "        with open(join(self.output_path, output_name, '.json'), \"r\") as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "        self.model = model_from_json(loaded_model_json)\n",
    "        self.model.load_weights(join(self.output_path, output_name, '.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using a generator (to avoid having to fit all the data in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "training_features = join('dataset', 'training_features')\n",
    "output_path = join('bin')\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.load(training_features, generate_binary_sequences=True)\n",
    "dataset.save_metadata(output_path)\n",
    "dataset.voc.save(output_path)\n",
    "\n",
    "gui_paths, img_paths = Dataset.load_paths_only(training_features)\n",
    "\n",
    "input_shape = dataset.input_shape\n",
    "output_size = dataset.output_size\n",
    "steps_per_epoch = dataset.size / BATCH_SIZE\n",
    "voc = Vocabulary()\n",
    "voc.retrieve(output_path)\n",
    "\n",
    "generator = Generator.data_generator(voc, gui_paths, img_paths, batch_size=BATCH_SIZE, generate_binary_sequences=True)\n",
    "\n",
    "model = magicode(input_shape, output_size, output_path)\n",
    "\n",
    "model.fit_generator(generator, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the directories for storing screenshots to \"decode\" and the resulting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory to store images to be \"decoded\"\n",
    "if not os.path.exists('screenshots_to_convert'):\n",
    "    os.mkdir('screenshots_to_convert')\n",
    "# create directory to store HTML and GUI code generated by \"decoding\" images in screenshots folder \n",
    "if not os.path.exists('generated_code'):\n",
    "    os.mkdir('generated_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the code for provided screenshots - add files to the screenshots folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights_path = 'bin'\n",
    "trained_model_name = 'magicode'\n",
    "input_path = 'screenshots_to_convert'\n",
    "output_path = 'generated_code'\n",
    "\n",
    "meta_dataset = np.load(join(trained_weights_path, 'meta_dataset.npy'), allow_pickle=True)\n",
    "input_shape = meta_dataset[0]\n",
    "output_size = meta_dataset[1]\n",
    "\n",
    "model = magicode(input_shape, output_size, trained_weights_path)\n",
    "model.load(trained_model_name)\n",
    "\n",
    "sampler = Sampler(trained_weights_path, input_shape, output_size, CONTEXT_LENGTH)\n",
    "\n",
    "for f in os.listdir(input_path):\n",
    "    if f.find('.png') != -1:\n",
    "        evaluation_img = Utils.get_preprocessed_img(join(input_path,f), IMAGE_SIZE)\n",
    "\n",
    "        file_name = f[:f.find('.png')]\n",
    "\n",
    "        result, _ = sampler.predict_greedy(model, np.array([evaluation_img]))\n",
    "        print('Result greedy: {}'.format(result))\n",
    "\n",
    "        with open(join(output_path, file_name + '.gui'), 'w') as out_f:\n",
    "            out_f.write(result.replace(START_TOKEN, '').replace(END_TOKEN, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class Compiler:\n",
    "    def __init__(self, dsl_mapping_file_path):\n",
    "        with open(dsl_mapping_file_path) as data_file:\n",
    "            self.dsl_mapping = json.load(data_file)\n",
    "\n",
    "        self.opening_tag = self.dsl_mapping['opening-tag']\n",
    "        self.closing_tag = self.dsl_mapping['closing-tag']\n",
    "        self.content_holder = self.opening_tag + self.closing_tag\n",
    "\n",
    "        self.root = Node('body', None, self.content_holder)\n",
    "\n",
    "    def compile(self, input_file_path, output_file_path, rendering_function=None):\n",
    "        dsl_file = open(input_file_path)\n",
    "        current_parent = self.root\n",
    "\n",
    "        for token in dsl_file:\n",
    "            token = token.replace(' ', '').replace('\\n', '')\n",
    "\n",
    "            if token.find(self.opening_tag) != -1:\n",
    "                token = token.replace(self.opening_tag, '')\n",
    "\n",
    "                element = Node(token, current_parent, self.content_holder)\n",
    "                current_parent.add_child(element)\n",
    "                current_parent = element\n",
    "            elif token.find(self.closing_tag) != -1:\n",
    "                current_parent = current_parent.parent\n",
    "            else:\n",
    "                tokens = token.split(',')\n",
    "                for t in tokens:\n",
    "                    element = Node(t, current_parent, self.content_holder)\n",
    "                    current_parent.add_child(element)\n",
    "\n",
    "        output_html = self.root.render(self.dsl_mapping, rendering_function=rendering_function)\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(output_html)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, key, parent_node, content_holder):\n",
    "        self.key = key\n",
    "        self.parent = parent_node\n",
    "        self.children = []\n",
    "        self.content_holder = content_holder\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def show(self):\n",
    "        print(self.key)\n",
    "        for child in self.children:\n",
    "            child.show()\n",
    "\n",
    "    def render(self, mapping, rendering_function=None):\n",
    "        content = ''\n",
    "        for child in self.children:\n",
    "            content += child.render(mapping, rendering_function)\n",
    "\n",
    "        value = mapping[self.key]\n",
    "        if rendering_function is not None:\n",
    "            value = rendering_function(self.key, value)\n",
    "\n",
    "        if len(self.children) != 0:\n",
    "            value = value.replace(self.content_holder, content)\n",
    "\n",
    "        return value\n",
    "\n",
    "class CompilerUtils:\n",
    "    @staticmethod\n",
    "    def get_random_text(length_text=10, space_number=1, with_upper_case=True):\n",
    "        results = []\n",
    "        while len(results) < length_text:\n",
    "            char = random.choice(string.ascii_letters[:26])\n",
    "            results.append(char)\n",
    "        if with_upper_case:\n",
    "            results[0] = results[0].upper()\n",
    "\n",
    "        current_spaces = []\n",
    "        while len(current_spaces) < space_number:\n",
    "            space_pos = random.randint(2, length_text - 3)\n",
    "            if space_pos in current_spaces:\n",
    "                break\n",
    "            results[space_pos] = \" \"\n",
    "            if with_upper_case:\n",
    "                results[space_pos + 1] = results[space_pos - 1].upper()\n",
    "\n",
    "            current_spaces.append(space_pos)\n",
    "\n",
    "        return ''.join(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the generated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_WITH_RANDOM_TEXT = True\n",
    "TEXT_PLACE_HOLDER = '[]'\n",
    "\n",
    "dsl_path = join('compiler','assets','dsl-mapping.json')\n",
    "compiler = Compiler(dsl_path)\n",
    "\n",
    "def render_content_with_text(key, value):\n",
    "    text_inputs = ['input-text', 'input-password']\n",
    "    control_inputs = ['input-checkbox', 'input-radio']\n",
    "    if FILL_WITH_RANDOM_TEXT:\n",
    "        if key.find('btn') != -1:\n",
    "            value = value.replace(TEXT_PLACE_HOLDER, CompilerUtils.get_random_text())\n",
    "        elif key.find('title') != -1:\n",
    "            value = value.replace(TEXT_PLACE_HOLDER, CompilerUtils.get_random_text(length_text=5, space_number=0))\n",
    "        elif key.find('text') != -1:\n",
    "            value = value.replace(TEXT_PLACE_HOLDER,\n",
    "                                  CompilerUtils.get_random_text(length_text=56, space_number=7, with_upper_case=False))\n",
    "        elif any(text_input in key for text_input in text_inputs):\n",
    "            value = value.replace(TEXT_PLACE_HOLDER, CompilerUtils.get_random_text(length_text=30, space_number=0))\n",
    "        elif any(control_input in key for control_input in control_inputs):\n",
    "            value = value.replace(TEXT_PLACE_HOLDER, CompilerUtils.get_random_text(length_text=10, space_number=0))\n",
    "    return value\n",
    "\n",
    "path = 'generated_code'\n",
    "generated_code_files = os.listdir(path)\n",
    "\n",
    "for file in generated_code_files:\n",
    "    file_uid = Path(file).stem\n",
    "    input_file_path = join(path, file_uid + '.gui')\n",
    "    output_file_path = join(path, file_uid + '.html')\n",
    "\n",
    "    compiler.compile(input_file_path, output_file_path, rendering_function=render_content_with_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
